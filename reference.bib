
@inproceedings{venkatesan_magnet_2019,
	address = {Westminster, CO, USA},
	title = {{MAGNet}: {A} {Modular} {Accelerator} {Generator} for {Neural} {Networks}},
	isbn = {978-1-72812-350-9},
	shorttitle = {{MAGNet}},
	url = {https://ieeexplore.ieee.org/document/8942127/},
	doi = {10.1109/ICCAD45719.2019.8942127},
	abstract = {Deep neural networks have been adopted in a wide range of application domains, leading to high demand for inference accelerators. However, the high cost associated with ASIC hardware design makes it challenging to build custom accelerators for different targets. To lower design cost, we propose MAGNet, a modular accelerator generator for neural networks. MAGNet takes a target application consisting of one or more neural networks along with hardware constraints as input and produces synthesizable RTL for a neural network accelerator ASIC as well as valid mappings for running the target networks on the generated hardware. MAGNet consists of three key components: (i) MAGNet Designer, a highly conﬁgurable architectural template designed in C++ and synthesizable by high-level synthesis tools. MAGNet Designer supports a wide range of design-time parameters such as different data formats, diverse memory hierarchies, and dataﬂows. (ii) MAGNet Mapper, an automated framework for exploring different software mappings for executing a neural network on the generated hardware. (iii) MAGNet Tuner, a design space exploration framework encompassing the designer, the mapper, and a deep learning framework to enable fast design space exploration and co-optimization of architecture and application. We demonstrate the utility of MAGNet by designing an inference accelerator optimized for image classiﬁcation application using three different neural networks—AlexNet, ResNet, and DriveNet. MAGNet-generated hardware is highly efﬁcient and leverages a novel multi-level dataﬂow to achieve 40 fJ/op and 2.8 TOPS/mm2 in a 16nm technology node for the ResNet-50 benchmark with {\textless}1\% accuracy loss on the ImageNet dataset.},
	language = {en},
	urldate = {2021-11-04},
	booktitle = {2019 {IEEE}/{ACM} {International} {Conference} on {Computer}-{Aided} {Design} ({ICCAD})},
	publisher = {IEEE},
	author = {Venkatesan, Rangharajan and Raina, Priyanka and Zhang, Yanqing and Zimmer, Brian and Dally, William J. and Emer, Joel and Keckler, Stephen W. and Khailany, Brucek and Shao, Yakun Sophia and Wang, Miaorong and Clemons, Jason and Dai, Steve and Fojtik, Matthew and Keller, Ben and Klinefelter, Alicia and Pinckney, Nathaniel},
	month = nov,
	year = {2019},
	pages = {1--8},
	file = {Venkatesan 等。 - 2019 - MAGNet A Modular Accelerator Generator for Neural.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\NRV3VS3K\\Venkatesan 等。 - 2019 - MAGNet A Modular Accelerator Generator for Neural.pdf:application/pdf},
}

@article{grosser_polyhedral_2015,
	title = {Polyhedral {AST} {Generation} {Is} {More} {Than} {Scanning} {Polyhedra}},
	volume = {37},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/2743016},
	doi = {10.1145/2743016},
	abstract = {Abstract mathematical representations such as integer polyhedra have been shown to be useful to precisely analyze computational kernels and to express complex loop transformations. Such transformations rely on abstract syntax tree (AST) generators to convert the mathematical representation back to an imperative program. Such generic AST generators avoid the need to resort to transformation-specific code generators, which may be very costly or technically difficult to develop as transformations become more complex. Existing AST generators have proven their effectiveness, but they hit limitations in more complex scenarios. Specifically, (1) they do not support or may fail to generate control flow for complex transformations using piecewise schedules or mappings involving modulo arithmetic; (2) they offer limited support for the specialization of the generated code exposing compact, straightline, vectorizable kernels with high arithmetic intensity necessary to exploit the peak performance of modern hardware; (3) they offer no support for memory layout transformations; and (4) they provide insufficient control over the AST generation strategy, preventing their application to complex domain-specific optimizations.
            We present a new AST generation approach that extends classical polyhedral scanning to the full generality of Presburger arithmetic, including existentially quantified variables and piecewise schedules, and introduce new optimizations for the detection of components and shifted strides. Not limiting ourselves to control flow generation, we expose functionality to generate AST expressions from arbitrary piecewise quasi-affine expressions, which enables the use of our AST generator for data-layout transformations. We complement this with support for specialization by polyhedral unrolling, user-directed versioning, and specialization of AST expressions according to the location at which they are generated, and we complete this work with fine-grained user control over the AST generation strategies used. Using this generalized idea of AST generation, we present how to implement complex domain-specific transformations without the need to write specialized code generators, but instead relying on a generic AST generator parametrized to a specific problem domain.},
	language = {en},
	number = {4},
	urldate = {2021-11-03},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert},
	month = aug,
	year = {2015},
	pages = {1--50},
	file = {Grosser 等。 - 2015 - Polyhedral AST Generation Is More Than Scanning Po.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\LYXYY2NH\\Grosser 等。 - 2015 - Polyhedral AST Generation Is More Than Scanning Po.pdf:application/pdf},
}

@article{verdoolaege_high-level_2017,
	title = {High-{Level} {Loop} {Transformations} and {Polyhedral} {Compilation}},
	language = {en},
	author = {Verdoolaege, Sven},
	year = {2017},
	pages = {24},
	file = {Verdoolaege - 2017 - High-Level Loop Transformations and Polyhedral Com.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\KVQ3RXQT\\Verdoolaege - 2017 - High-Level Loop Transformations and Polyhedral Com.pdf:application/pdf},
}

@article{verdoolaege_schedule_2014,
	title = {Schedule {Trees}},
	url = {http://rgdoi.net/10.13140/RG.2.1.4475.6001},
	doi = {10.13140/RG.2.1.4475.6001},
	language = {en},
	urldate = {2021-11-02},
	author = {Verdoolaege, Sven and Guelton, Serge and Grosser, Tobias and Cohen, Albert},
	year = {2014},
	note = {Publisher: Unpublished},
	file = {34599567.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\ME7REAP8\\34599567.pdf:application/pdf},
}

@inproceedings{ragan-kelley_halide_2013,
	address = {Seattle, Washington, USA},
	title = {Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
	isbn = {978-1-4503-2014-6},
	shorttitle = {Halide},
	url = {http://dl.acm.org/citation.cfm?doid=2491956.2462176},
	doi = {10.1145/2491956.2462176},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} conference on {Programming} language design and implementation - {PLDI} '13},
	publisher = {ACM Press},
	author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Frédo and Amarasinghe, Saman},
	year = {2013},
	pages = {519},
	file = {Ragan-Kelley 等。 - 2013 - Halide a language and compiler for optimizing par.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\RQFABCFW\\Ragan-Kelley 等。 - 2013 - Halide a language and compiler for optimizing par.pdf:application/pdf},
}

@inproceedings{lai_susy_2020,
	address = {Virtual Event USA},
	title = {{SuSy}: a programming model for productive construction of high-performance systolic arrays on {FPGAs}},
	isbn = {978-1-4503-8026-3},
	shorttitle = {{SuSy}},
	url = {https://dl.acm.org/doi/10.1145/3400302.3415644},
	doi = {10.1145/3400302.3415644},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Computer}-{Aided} {Design}},
	publisher = {ACM},
	author = {Lai, Yi-Hsiang and Rong, Hongbo and Zheng, Size and Zhang, Weihao and Cui, Xiuping and Jia, Yunshan and Wang, Jie and Sullivan, Brendan and Zhang, Zhiru and Liang, Yun and Zhang, Youhui and Cong, Jason and George, Nithin and Alvarez, Jose and Hughes, Christopher and Dubey, Pradeep},
	month = nov,
	year = {2020},
	pages = {1--9},
	file = {Lai 等。 - 2020 - SuSy a programming model for productive construct.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\GUI4JIMD\\Lai 等。 - 2020 - SuSy a programming model for productive construct.pdf:application/pdf},
}

@inproceedings{ramanujam_automatic_2007,
	address = {San Jose, California, USA},
	title = {Automatic mapping of nested loops to {FPGAS}},
	isbn = {978-1-59593-602-8},
	url = {http://portal.acm.org/citation.cfm?doid=1229428.1229446},
	doi = {10.1145/1229428.1229446},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the 12th {ACM} {SIGPLAN} symposium on {Principles} and practice of parallel programming  - {PPoPP} '07},
	publisher = {ACM Press},
	author = {Ramanujam, J. and Sadayappan, P.},
	year = {2007},
	pages = {101},
	file = {全文:C\:\\Users\\Armeria\\Zotero\\storage\\5YBYWXE7\\Ramanujam 和 Sadayappan - 2007 - Automatic mapping of nested loops to FPGAS.pdf:application/pdf},
}

@inproceedings{wang_autosa_2021,
	address = {Virtual Event USA},
	title = {{AutoSA}: {A} {Polyhedral} {Compiler} for {High}-{Performance} {Systolic} {Arrays} on {FPGA}},
	isbn = {978-1-4503-8218-2},
	shorttitle = {{AutoSA}},
	url = {https://dl.acm.org/doi/10.1145/3431920.3439292},
	doi = {10.1145/3431920.3439292},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {The 2021 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {ACM},
	author = {Wang, Jie and Guo, Licheng and Cong, Jason},
	month = feb,
	year = {2021},
	pages = {93--104},
	file = {全文:C\:\\Users\\Armeria\\Zotero\\storage\\ZWFYRDZU\\Wang 等。 - 2021 - AutoSA A Polyhedral Compiler for High-Performance.pdf:application/pdf},
}

@article{verdoolaege_polyhedral_2013,
	title = {Polyhedral parallel code generation for {CUDA}},
	volume = {9},
	issn = {1544-3566, 1544-3973},
	url = {https://dl.acm.org/doi/10.1145/2400682.2400713},
	doi = {10.1145/2400682.2400713},
	abstract = {This article addresses the compilation of a sequential program for parallel execution on a modern GPU. To this end, we present a novel source-to-source compiler called PPCG. PPCG singles out for its ability to accelerate computations from any static control loop nest, generating multiple CUDA kernels when necessary. We introduce a multilevel tiling strategy and a code generation scheme for the parallelization and locality optimization of imperfectly nested loops, managing memory and exposing concurrency according to the constraints of modern GPUs. We evaluate our algorithms and tool on the entire PolyBench suite.},
	language = {en},
	number = {4},
	urldate = {2021-11-02},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Verdoolaege, Sven and Carlos Juega, Juan and Cohen, Albert and Ignacio Gómez, José and Tenllado, Christian and Catthoor, Francky},
	month = jan,
	year = {2013},
	pages = {1--23},
	file = {全文:C\:\\Users\\Armeria\\Zotero\\storage\\TYEUNTG6\\Verdoolaege 等。 - 2013 - Polyhedral parallel code generation for CUDA.pdf:application/pdf},
}

@incollection{rauchwerger_putting_2004,
	address = {Berlin, Heidelberg},
	title = {Putting {Polyhedral} {Loop} {Transformations} to {Work}},
	volume = {2958},
	isbn = {978-3-540-21199-0 978-3-540-24644-2},
	url = {http://link.springer.com/10.1007/978-3-540-24644-2_14},
	abstract = {We seek to extend the scope and efﬁciency of iterative compilation techniques by searching not only for program transformation parameters but for the most appropriate transformations themselves. For that purpose, we need a generic way to express program transformations and compositions of transformations. In this article, we introduce a framework for the polyhedral representation of a wide range of transformations in a uniﬁed way. We also show that it is possible to generate efﬁcient code after the application of polyhedral program transformations. Finally, we demonstrate an implementation of the polyhedral representation and code generation techniques in the Open64/ORC compiler.},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {Languages and {Compilers} for {Parallel} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bastoul, Cédric and Cohen, Albert and Girbal, Sylvain and Sharma, Saurabh and Temam, Olivier},
	editor = {Rauchwerger, Lawrence},
	year = {2004},
	doi = {10.1007/978-3-540-24644-2_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {209--225},
	file = {BCGST03-LCPC[1].pdf:C\:\\Users\\Armeria\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\JY4P6L4T\\BCGST03-LCPC[1].pdf:application/pdf},
}

@article{kung_why_1982,
	title = {Why systolic architectures?},
	volume = {15},
	issn = {0018-9162},
	url = {http://ieeexplore.ieee.org/document/1653825/},
	doi = {10.1109/MC.1982.1653825},
	language = {en},
	number = {1},
	urldate = {2021-11-02},
	journal = {Computer},
	author = {{Kung}},
	month = jan,
	year = {1982},
	pages = {37--46},
	file = {kung_-_1982_-_why_systolic_architectures[1].pdf:C\:\\Users\\Armeria\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\PTLDVD49\\kung_-_1982_-_why_systolic_architectures[1].pdf:application/pdf},
}

@article{bondhugula_high_2020,
	title = {High {Performance} {Code} {Generation} in {MLIR}: {An} {Early} {Case} {Study} with {GEMM}},
	shorttitle = {High {Performance} {Code} {Generation} in {MLIR}},
	url = {http://arxiv.org/abs/2003.00532},
	abstract = {This article is primarily meant to present an early case study on using MLIR, a new compiler intermediate representation infrastructure, for high-performance code generation. Aspects of MLIR covered in particular include memrefs, the affine dialect, and polyhedral utilities and pass infrastructure surrounding those. This article is also aimed at showing the role compiler infrastructure could play in generating code that is competitive with highly tuned manually developed libraries, albeit in a more modular, reusable, and automatable way.},
	urldate = {2021-11-02},
	journal = {arXiv:2003.00532 [cs]},
	author = {Bondhugula, Uday},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.00532},
	keywords = {Computer Science - Performance},
	file = {arXiv.org Snapshot:C\:\\Users\\Armeria\\Zotero\\storage\\Y3VJQK5P\\2003.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Armeria\\Zotero\\storage\\QXWY92N8\\Bondhugula - 2020 - High Performance Code Generation in MLIR An Early.pdf:application/pdf},
}

@article{lattner_mlir_2020,
	title = {{MLIR}: {A} {Compiler} {Infrastructure} for the {End} of {Moore}'s {Law}},
	shorttitle = {{MLIR}},
	url = {http://arxiv.org/abs/2002.11054},
	abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
	urldate = {2021-11-02},
	journal = {arXiv:2002.11054 [cs]},
	author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.11054},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:C\:\\Users\\Armeria\\Zotero\\storage\\6X3W7UU3\\2002.html:text/html;arXiv Fulltext PDF:C\:\\Users\\Armeria\\Zotero\\storage\\PWFPHNUE\\Lattner 等。 - 2020 - MLIR A Compiler Infrastructure for the End of Moo.pdf:application/pdf},
}

@inproceedings{chen_eyeriss_2016,
	address = {Seoul, South Korea},
	title = {Eyeriss: {A} {Spatial} {Architecture} for {Energy}-{Efficient} {Dataflow} for {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8947-1},
	shorttitle = {Eyeriss},
	url = {http://ieeexplore.ieee.org/document/7551407/},
	doi = {10.1109/ISCA.2016.40},
	abstract = {Deep convolutional neural networks (CNNs) are widely used in modern AI systems for their superior accuracy but at the cost of high computational complexity. The complexity comes from the need to simultaneously process hundreds of ﬁlters and channels in the high-dimensional convolutions, which involve a signiﬁcant amount of data movement. Although highly-parallel compute paradigms, such as SIMD/SIMT, effectively address the computation requirement to achieve high throughput, energy consumption still remains high as data movement can be more expensive than computation. Accordingly, ﬁnding a dataﬂow that supports parallel processing with minimal data movement cost is crucial to achieving energyefﬁcient CNN processing without compromising accuracy. In this paper, we present a novel dataﬂow, called rowstationary (RS), that minimizes data movement energy consumption on a spatial architecture. This is realized by exploiting local data reuse of ﬁlter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations. Unlike dataﬂows used in existing designs, which only reduce certain types of data movement, the proposed RS dataﬂow can adapt to different CNN shape conﬁgurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism. To evaluate the energy efﬁciency of the different dataﬂows, we propose an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints. Experiments using the CNN conﬁgurations of AlexNet show that the proposed RS dataﬂow is more energy efﬁcient than existing dataﬂows in both convolutional (1.4× to 2.5×) and fully-connected layers (at least 1.3× for batch size larger than 16). The RS dataﬂow has also been demonstrated on a fabricated chip, which veriﬁes our energy analysis.},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {2016 {ACM}/{IEEE} 43rd {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	publisher = {IEEE},
	author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
	month = jun,
	year = {2016},
	pages = {367--379},
	file = {Chen 等。 - 2016 - Eyeriss A Spatial Architecture for Energy-Efficie.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\6SB3VVVM\\Chen 等。 - 2016 - Eyeriss A Spatial Architecture for Energy-Efficie.pdf:application/pdf},
}

@inproceedings{cong_polysa_2018,
	address = {San Diego California},
	title = {{PolySA}: polyhedral-based systolic array auto-compilation},
	isbn = {978-1-4503-5950-4},
	shorttitle = {{PolySA}},
	url = {https://dl.acm.org/doi/10.1145/3240765.3240838},
	doi = {10.1145/3240765.3240838},
	abstract = {Automatic systolic array generation has long been an interesting topic due to the need to reduce the lengthy development cycles of manual designs. Existing automatic systolic array generation approach builds dependency graphs from algorithms, and iteratively maps computation nodes in the graph into processing elements (PEs) with time stamps that specify the sequences of nodes that operate within the PE. There are a number of previous works that implemented the idea and generated designs for ASICs. However, all of these works relied on human intervention and usually generated inferior designs compared to manual designs. In this work, we present our ongoing compilation framework named PolySA which leverages the power of the polyhedral model to achieve the end-to-end compilation for systolic array architecture on FPGAs. PolySA is the first fully automated compilation framework for generating high-performance systolic array architectures on the FPGA leveraging recent advances in high-level synthesis. We demonstrate PolySA on two key applications—matrix multiplication and convolutional neural network. PolySA is able to generate optimal designs within one hour with performance comparable to state-of-the-art manual designs.},
	language = {en},
	urldate = {2021-11-02},
	booktitle = {Proceedings of the {International} {Conference} on {Computer}-{Aided} {Design}},
	publisher = {ACM},
	author = {Cong, Jason and Wang, Jie},
	month = nov,
	year = {2018},
	pages = {1--8},
	file = {Cong 和 Wang - 2018 - PolySA polyhedral-based systolic array auto-compi.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\NAM9WCYV\\Cong 和 Wang - 2018 - PolySA polyhedral-based systolic array auto-compi.pdf:application/pdf},
}

@inproceedings{pouchet_polyhedral-based_2013,
	address = {Monterey, California, USA},
	title = {Polyhedral-based data reuse optimization for configurable computing},
	isbn = {978-1-4503-1887-7},
	url = {http://dl.acm.org/citation.cfm?doid=2435264.2435273},
	doi = {10.1145/2435264.2435273},
	abstract = {Many applications, such as medical imaging, generate intensive data trafﬁc between the FPGA and off-chip memory. Signiﬁcant improvements in the execution time can be achieved with effective utilization of on-chip (scratchpad) memories, associated with careful software-based data reuse and communication scheduling techniques.},
	language = {en},
	urldate = {2021-11-13},
	booktitle = {Proceedings of the {ACM}/{SIGDA} international symposium on {Field} programmable gate arrays - {FPGA} '13},
	publisher = {ACM Press},
	author = {Pouchet, Louis-Noel and Zhang, Peng and Sadayappan, P. and Cong, Jason},
	year = {2013},
	pages = {29},
	file = {Pouchet 等。 - 2013 - Polyhedral-based data reuse optimization for confi.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\B22A5B3L\\Pouchet 等。 - 2013 - Polyhedral-based data reuse optimization for confi.pdf:application/pdf},
}

@article{mei_zigzag_2021,
	title = {{ZigZag}: {Enlarging} {Joint} {Architecture}-{Mapping} {Design} {Space} {Exploration} for {DNN} {Accelerators}},
	volume = {70},
	issn = {0018-9340, 1557-9956, 2326-3814},
	shorttitle = {{ZigZag}},
	url = {https://ieeexplore.ieee.org/document/9360462/},
	doi = {10.1109/TC.2021.3059962},
	abstract = {Building efﬁcient embedded deep learning systems requires a tight co-design between DNN algorithms, hardware, and algorithm-to-hardware mapping, a.k.a. dataﬂow. However, owing to the large joint design space, ﬁnding an optimal solution through physical implementation becomes infeasible. To tackle this problem, several design space exploration (DSE) frameworks have emerged recently, yet they either suffer from long runtimes or a limited exploration space. This article introduces ZigZag, a rapid DSE framework for DNN accelerator architecture and mapping. ZigZag extends the common DSE with uneven mapping opportunities and smart mapping search strategies. Uneven mapping decouples operands (W/I/O), memory hierarchy, and mappings (temporal/spatial), opening up a whole new space for DSE, and thus better design points are found by ZigZag compared to other SotAs. For this, ZigZag uses an enhanced nested-for-loop format as a uniform representation to integrate algorithm, accelerator, and algorithm-to-accelerator mapping. ZigZag consists of three key components: 1) an analytical energy-performance-area Hardware Cost Estimator, 2) two Mapping Search Engines that support spatial and temporal even/uneven mapping search, and 3) an Architecture Generator that auto-explores the wide memory hierarchy design space. Benchmarking experiments against published works, in-house accelerator, and existing DSE frameworks, together with three case studies, show the reliability and capability of ZigZag. Up to 64 percent more energy-efﬁcient solutions are found compared to other SotAs, due to ZigZag’s uneven mapping capabilities.},
	language = {en},
	number = {8},
	urldate = {2021-11-13},
	journal = {IEEE Transactions on Computers},
	author = {Mei, Linyan and Houshmand, Pouya and Jain, Vikram and Giraldo, Sebastian and Verhelst, Marian},
	month = aug,
	year = {2021},
	pages = {1160--1174},
	file = {Mei 等。 - 2021 - ZigZag Enlarging Joint Architecture-Mapping Desig.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\65SUXUL5\\Mei 等。 - 2021 - ZigZag Enlarging Joint Architecture-Mapping Desig.pdf:application/pdf},
}

@article{xi_smaug_2020,
	title = {{SMAUG}: {End}-to-{End} {Full}-{Stack} {Simulation} {Infrastructure} for {Deep} {Learning} {Workloads}},
	volume = {17},
	issn = {1544-3566, 1544-3973},
	shorttitle = {{SMAUG}},
	url = {https://dl.acm.org/doi/10.1145/3424669},
	doi = {10.1145/3424669},
	abstract = {In recent years, there has been tremendous advances in hardware acceleration of deep neural networks. However, most of the research has focused on optimizing accelerator microarchitecture for higher performance and energy efficiency on a per-layer basis. We find that for overall single-batch inference latency, the accelerator may only make up 25–40\%, with the rest spent on data movement and in the deep learning software framework. Thus far, it has been very difficult to study end-to-end DNN performance during early stage design (before RTL is available), because there are no existing DNN frameworks that support end-to-end simulation with easy custom hardware accelerator integration. To address this gap in research infrastructure, we present SMAUG, the first DNN framework that is purpose-built for simulation of end-to-end deep learning applications. SMAUG offers researchers a wide range of capabilities for evaluating DNN workloads, from diverse network topologies to easy accelerator modeling and SoC integration. To demonstrate the power and value of SMAUG, we present case studies that show how we can optimize overall performance and energy efficiency for up to 1.8×–5× speedup over a baseline system, without changing any part of the accelerator microarchitecture, as well as show how SMAUG can tune an SoC for a camera-powered deep learning pipeline.},
	language = {en},
	number = {4},
	urldate = {2021-11-13},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Xi, Sam (Likun) and Yao, Yuan and Bhardwaj, Kshitij and Whatmough, Paul and Wei, Gu-Yeon and Brooks, David},
	month = dec,
	year = {2020},
	pages = {1--26},
	file = {Xi 等。 - 2020 - SMAUG End-to-End Full-Stack Simulation Infrastruc.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\VFAI4QZW\\Xi 等。 - 2020 - SMAUG End-to-End Full-Stack Simulation Infrastruc.pdf:application/pdf},
}

@article{capra_hardware_2020,
	title = {Hardware and {Software} {Optimizations} for {Accelerating} {Deep} {Neural} {Networks}: {Survey} of {Current} {Trends}, {Challenges}, and the {Road} {Ahead}},
	volume = {8},
	issn = {2169-3536},
	shorttitle = {Hardware and {Software} {Optimizations} for {Accelerating} {Deep} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/9269334/},
	doi = {10.1109/ACCESS.2020.3039858},
	abstract = {Currently, Machine Learning (ML) is becoming ubiquitous in everyday life. Deep Learning (DL) is already present in many applications ranging from computer vision for medicine to autonomous driving of modern cars as well as other sectors in security, healthcare, and ﬁnance. However, to achieve impressive performance, these algorithms employ very deep networks, requiring a signiﬁcant computational power, both during the training and inference time. A single inference of a DL model may require billions of multiply-and-accumulated operations, making the DL extremely compute- and energy-hungry. In a scenario where several sophisticated algorithms need to be executed with limited energy and low latency, the need for cost-effective hardware platforms capable of implementing energy-efﬁcient DL execution arises. This paper ﬁrst introduces the key properties of two brain-inspired models like Deep Neural Network (DNN), and Spiking Neural Network (SNN), and then analyzes techniques to produce efﬁcient and high-performance designs. This work summarizes and compares the works for four leading platforms for the execution of algorithms such as CPU, GPU, FPGA and ASIC describing the main solutions of the state-of-the-art, giving much prominence to the last two solutions since they offer greater design ﬂexibility and bear the potential of high energy-efﬁciency, especially for the inference process. In addition to hardware solutions, this paper discusses some of the important security issues that these DNN and SNN models may have during their execution, and offers a comprehensive section on benchmarking, explaining how to assess the quality of different networks and hardware systems designed for them.},
	language = {en},
	urldate = {2021-11-13},
	journal = {IEEE Access},
	author = {Capra, Maurizio and Bussolino, Beatrice and Marchisio, Alberto and Masera, Guido and Martina, Maurizio and Shafique, Muhammad},
	year = {2020},
	pages = {225134--225180},
	file = {Capra 等。 - 2020 - Hardware and Software Optimizations for Accelerati.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\MM5XAF4X\\Capra 等。 - 2020 - Hardware and Software Optimizations for Accelerati.pdf:application/pdf},
}

@article{bondhugula_practical_nodate,
	title = {A {Practical} {Automatic} {Polyhedral} {Parallelizer} and {Locality} {Optimizer}},
	abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model.Unlike previous polyhedral frameworks, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of ﬁnding good ways of tiling for parallelism and locality using afﬁne transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high performance for local and parallel execution on multi-cores, when compared with state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
	language = {en},
	author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J and Sadayappan, P},
	pages = {13},
	file = {Bondhugula 等。 - A Practical Automatic Polyhedral Parallelizer and .pdf:C\:\\Users\\Armeria\\Zotero\\storage\\7EY8A4RU\\Bondhugula 等。 - A Practical Automatic Polyhedral Parallelizer and .pdf:application/pdf},
}

@incollection{hutchison_polyhedral_2010,
	address = {Berlin, Heidelberg},
	title = {The {Polyhedral} {Model} {Is} {More} {Widely} {Applicable} {Than} {You} {Think}},
	volume = {6011},
	isbn = {978-3-642-11969-9 978-3-642-11970-5},
	url = {http://link.springer.com/10.1007/978-3-642-11970-5_16},
	abstract = {The polyhedral model is a powerful framework for automatic optimization and parallelization. It is based on an algebraic representation of programs, allowing to construct and search for complex sequences of optimizations. This model is now mature and reaches production compilers. The main limitation of the polyhedral model is known to be its restriction to statically predictable, loop-based program parts. This paper removes this limitation, allowing to operate on general data-dependent control-ﬂow. We embed control and exit predicates as ﬁrst-class citizens of the algebraic representation, from program analysis to code generation. Complementing previous (partial) attempts in this direction, our work concentrates on extending the code generation step and does not compromise the expressiveness of the model. We present experimental evidence that our extension is relevant for program optimization and parallelization, showing performance improvements on benchmarks that were thought to be out of reach of the polyhedral model.},
	language = {en},
	urldate = {2021-11-13},
	booktitle = {Compiler {Construction}},
	publisher = {Springer Berlin Heidelberg},
	author = {Benabderrahmane, Mohamed-Walid and Pouchet, Louis-Noël and Cohen, Albert and Bastoul, Cédric},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gupta, Rajiv},
	year = {2010},
	doi = {10.1007/978-3-642-11970-5_16},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {283--303},
	file = {Benabderrahmane 等。 - 2010 - The Polyhedral Model Is More Widely Applicable Tha.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\VPHRSQYN\\Benabderrahmane 等。 - 2010 - The Polyhedral Model Is More Widely Applicable Tha.pdf:application/pdf},
}

@article{genc_gemmini_2021,
	title = {Gemmini: {Enabling} {Systematic} {Deep}-{Learning} {Architecture} {Evaluation} via {Full}-{Stack} {Integration}},
	shorttitle = {Gemmini},
	url = {http://arxiv.org/abs/1911.09925},
	abstract = {DNN accelerators are often developed and evaluated in isolation without considering the cross-stack, system-level effects in real-world environments. This makes it difﬁcult to appreciate the impact of Systemon-Chip (SoC) resource contention, OS overheads, and programmingstack inefﬁciencies on overall performance/energy-efﬁciency. To address this challenge, we present Gemmini, an open-source1, full-stack DNN accelerator generator. Gemmini generates a wide design-space of efﬁcient ASIC accelerators from a ﬂexible architectural template, together with ﬂexible programming stacks and full SoCs with shared resources that capture system-level effects. Gemmini-generated accelerators have also been fabricated, delivering up to three orders-of-magnitude speedups over high-performance CPUs on various DNN benchmarks.},
	language = {en},
	urldate = {2021-11-12},
	journal = {arXiv:1911.09925 [cs]},
	author = {Genc, Hasan and Kim, Seah and Amid, Alon and Haj-Ali, Ameer and Iyer, Vighnesh and Prakash, Pranav and Zhao, Jerry and Grubb, Daniel and Liew, Harrison and Mao, Howard and Ou, Albert and Schmidt, Colin and Steffl, Samuel and Wright, John and Stoica, Ion and Ragan-Kelley, Jonathan and Asanovic, Krste and Nikolic, Borivoje and Shao, Yakun Sophia},
	month = jul,
	year = {2021},
	note = {arXiv: 1911.09925},
	keywords = {Computer Science - Performance, Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
	file = {Genc 等。 - 2021 - Gemmini Enabling Systematic Deep-Learning Archite.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\AFMDGKHH\\Genc 等。 - 2021 - Gemmini Enabling Systematic Deep-Learning Archite.pdf:application/pdf},
}

@article{khailany_accelerating_2020,
	title = {Accelerating {Chip} {Design} {With} {Machine} {Learning}},
	volume = {40},
	issn = {0272-1732, 1937-4143},
	url = {https://ieeexplore.ieee.org/document/9205654/},
	doi = {10.1109/MM.2020.3026231},
	abstract = {Recent advancements in machine learning provide an opportunity to transform chip design workﬂows. We review recent research applying techniques such as deep convolutional neural networks and graph-based neural networks in the areas of automatic design space exploration, power analysis, VLSI physical design, and analog design. We also present a future vision of an AI-assisted automated chip design workﬂow to aid designer productivity and automate optimization tasks.},
	language = {en},
	number = {6},
	urldate = {2021-11-12},
	journal = {IEEE Micro},
	author = {Khailany, Brucek and Ren, Haoxing and Dai, Steve and Godil, Saad and Keller, Ben and Kirby, Robert and Klinefelter, Alicia and Venkatesan, Rangharajan and Zhang, Yanqing and Catanzaro, Bryan and Dally, William J.},
	month = nov,
	year = {2020},
	pages = {23--32},
	file = {Khailany 等。 - 2020 - Accelerating Chip Design With Machine Learning.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\2PU3LMDA\\Khailany 等。 - 2020 - Accelerating Chip Design With Machine Learning.pdf:application/pdf},
}

@inproceedings{imani_deep_2020,
	address = {San Diego, CA, USA},
	title = {Deep {Learning} {Acceleration} with {Neuron}-to-{Memory} {Transformation}},
	isbn = {978-1-72816-149-5},
	url = {https://ieeexplore.ieee.org/document/9065564/},
	doi = {10.1109/HPCA47549.2020.00011},
	abstract = {Deep neural networks (DNN) have demonstrated effectiveness for various applications such as image processing, video segmentation, and speech recognition. Running state-of-theart DNNs on current systems mostly relies on either generalpurpose processors, ASIC designs, or FPGA accelerators, all of which suffer from data movements due to the limited onchip memory and data transfer bandwidth. In this work, we propose a novel framework, called RAPIDNN, which performs neuron-to-memory transformation in order to accelerate DNNs in a highly parallel architecture. RAPIDNN reinterprets a DNN model and maps it into a specialized accelerator, which is designed using non-volatile memory blocks that model four fundamental DNN operations, i.e., multiplication, addition, activation functions, and pooling. The framework extracts representative operands of a DNN model, e.g., weights and input values, using clustering methods to optimize the model for in-memory processing. Then, it maps the extracted operands and their pre-computed results into the accelerator memory blocks. At runtime, the accelerator identiﬁes computation results based on efﬁcient in-memory search capability which also provides tunability of approximation to improve computation efﬁciency further. Our evaluation shows that RAPIDNN achieves 68.4×, 49.5× energy efﬁciency improvement and 48.1×, 10.9× speedup as compared to ISAAC and PipeLayer, the state-of-the-art DNN accelerators, while ensuring less than 0.5\% quality loss.},
	language = {en},
	urldate = {2021-11-12},
	booktitle = {2020 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	publisher = {IEEE},
	author = {Imani, Mohsen and Samragh Razlighi, Mohammad and Kim, Yeseong and Gupta, Saransh and Koushanfar, Farinaz and Rosing, Tajana},
	month = feb,
	year = {2020},
	pages = {1--14},
	file = {Imani 等。 - 2020 - Deep Learning Acceleration with Neuron-to-Memory T.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\8LUUJWJU\\Imani 等。 - 2020 - Deep Learning Acceleration with Neuron-to-Memory T.pdf:application/pdf},
}

@inproceedings{moss_customizable_2018,
	address = {Monterey CALIFORNIA USA},
	title = {A {Customizable} {Matrix} {Multiplication} {Framework} for the {Intel} {HARPv2} {Xeon}+{FPGA} {Platform}: {A} {Deep} {Learning} {Case} {Study}},
	isbn = {978-1-4503-5614-5},
	shorttitle = {A {Customizable} {Matrix} {Multiplication} {Framework} for the {Intel} {HARPv2} {Xeon}+{FPGA} {Platform}},
	url = {https://dl.acm.org/doi/10.1145/3174243.3174258},
	doi = {10.1145/3174243.3174258},
	language = {en},
	urldate = {2021-11-15},
	booktitle = {Proceedings of the 2018 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {ACM},
	author = {Moss, Duncan J.M and Krishnan, Srivatsan and Nurvitadhi, Eriko and Ratuszniak, Piotr and Johnson, Chris and Sim, Jaewoong and Mishra, Asit and Marr, Debbie and Subhaschandra, Suchit and Leong, Philip H.W.},
	month = feb,
	year = {2018},
	pages = {107--116},
}

@inproceedings{jouppi_-datacenter_2017,
	address = {Toronto ON Canada},
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {Unit}},
	isbn = {978-1-4503-4892-8},
	url = {https://dl.acm.org/doi/10.1145/3079856.3080246},
	doi = {10.1145/3079856.3080246},
	language = {en},
	urldate = {2021-11-15},
	booktitle = {Proceedings of the 44th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
	month = jun,
	year = {2017},
	pages = {1--12},
	file = {全文:C\:\\Users\\Armeria\\Zotero\\storage\\6IDSQLSR\\Jouppi 等。 - 2017 - In-Datacenter Performance Analysis of a Tensor Pro.pdf:application/pdf},
}

@article{rauer_accelerating_nodate,
	title = {Accelerating {Genomics} {Research} with {OpenCL}™ and {FPGAs}},
	url = {https://www.intel.com/content/dam/www/programmable/us/en/pdfs/literature/wp/wp-accelerating-genomics-opencl-fpgas.pdf},
	abstract = {With the rapid decrease in gene sequencing costs due to the emergence of secondgeneration sequencing equipment, the availability of genome sequence data is increasing dramatically. The ability to correlate the variations among genomes is enabling advances in a wide range of medical research and personalized care. Because each human genome comprises more than three billion base pairs, whole genomic sequencing requires significant processing power, storage capacity, and network bandwidth. In particular, variant calling is extremely computationally intensive. The Genome Analysis Toolkit (GATK) is a software package developed at the Broad Institute to analyze high-throughput sequencing data. This paper describes the acceleration of the GATK’s HaplotypeCaller algorithm using Intel’s field programmable gate array (FPGA) devices, programmed using the Intel® FPGA SDK for OpenCL™ technology.},
	language = {en},
	author = {Rauer, Chris and Powley, George S and Ahsan, Mir and Finamore, Nicholas},
	pages = {6},
	file = {Rauer 等。 - Accelerating Genomics Research with OpenCL™ and FP.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\U2U6FRDH\\Rauer 等。 - Accelerating Genomics Research with OpenCL™ and FP.pdf:application/pdf},
}

@article{chen_eyeriss_2019,
	title = {Eyeriss v2: {A} {Flexible} {Accelerator} for {Emerging} {Deep} {Neural} {Networks} on {Mobile} {Devices}},
	volume = {9},
	issn = {2156-3357, 2156-3365},
	shorttitle = {Eyeriss v2},
	url = {https://ieeexplore.ieee.org/document/8686088/},
	doi = {10.1109/JETCAS.2019.2910232},
	number = {2},
	urldate = {2021-11-15},
	journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	author = {Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S. and Sze, Vivienne},
	month = jun,
	year = {2019},
	pages = {292--308},
	file = {已提交版本:C\:\\Users\\Armeria\\Zotero\\storage\\IMDB34P9\\Chen 等。 - 2019 - Eyeriss v2 A Flexible Accelerator for Emerging De.pdf:application/pdf},
}

@inproceedings{farshchi_integrating_2019,
	address = {Washington, DC, USA},
	title = {Integrating {NVIDIA} {Deep} {Learning} {Accelerator} ({NVDLA}) with {RISC}-{V} {SoC} on {FireSim}},
	isbn = {978-1-72816-763-3},
	url = {https://ieeexplore.ieee.org/document/9027215/},
	doi = {10.1109/EMC249363.2019.00012},
	urldate = {2021-11-15},
	booktitle = {2019 2nd {Workshop} on {Energy} {Efficient} {Machine} {Learning} and {Cognitive} {Computing} for {Embedded} {Applications} ({EMC2})},
	publisher = {IEEE},
	author = {Farshchi, Farzad and Huang, Qijing and Yun, Heechul},
	month = feb,
	year = {2019},
	pages = {21--25},
	file = {已提交版本:C\:\\Users\\Armeria\\Zotero\\storage\\P9IUWUPP\\Farshchi 等。 - 2019 - Integrating NVIDIA Deep Learning Accelerator (NVDL.pdf:application/pdf},
}

@webpage{nvidia_nvdla_2018,
	title = {{NVDLA}},
	url = {http://nvdla.org/},
	abstract = {The NVIDIA Deep Learning Accelerator (NVDLA) is a free and open architecture that promotes a standard way to design deep learning inference accelerators. With its modular architecture, NVDLA is scalable, highly configurable, and designed to simplify integration and portability. The hardware supports a wide range of IoT devices. Delivered as an open source project under the NVIDIA Open NVDLA License, all of the software, hardware, and documentation will be available on GitHub. Contributions are welcome.},
	journal = {NVDLA},
	author = {NVIDIA},
	year = {2018},
}

@article{chen_using_2017,
	title = {Using {Dataflow} to {Optimize} {Energy} {Efficiency} of {Deep} {Neural} {Network} {Accelerators}},
	volume = {37},
	issn = {0272-1732},
	url = {http://ieeexplore.ieee.org/document/7948671/},
	doi = {10.1109/MM.2017.54},
	number = {3},
	urldate = {2021-11-15},
	journal = {IEEE Micro},
	author = {Chen, Yu-Hsin and Emer, Joel and Sze, Vivienne},
	year = {2017},
	pages = {12--21},
	file = {全文:C\:\\Users\\Armeria\\Zotero\\storage\\2LVD4A49\\Chen 等。 - 2017 - Using Dataflow to Optimize Energy Efficiency of De.pdf:application/pdf},
}

@inproceedings{gokhale_240_2014,
	address = {Columbus, OH, USA},
	title = {A 240 {G}-ops/s {Mobile} {Coprocessor} for {Deep} {Neural} {Networks}},
	isbn = {978-1-4799-4308-1},
	url = {http://ieeexplore.ieee.org/document/6910056/},
	doi = {10.1109/CVPRW.2014.106},
	urldate = {2021-11-15},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	publisher = {IEEE},
	author = {Gokhale, Vinayak and Jin, Jonghoon and Dundar, Aysegul and Martini, Berin and Culurciello, Eugenio},
	month = jun,
	year = {2014},
	pages = {696--701},
}

@article{yang_interstellar_2020,
	title = {Interstellar: {Using} {Halide}'s {Scheduling} {Language} to {Analyze} {DNN} {Accelerators}},
	shorttitle = {Interstellar},
	url = {http://arxiv.org/abs/1809.04070},
	doi = {10.1145/3373376.3378514},
	abstract = {We show that DNN accelerator micro-architectures and their program mappings represent specific choices of loop order and hardware parallelism for computing the seven nested loops of DNNs, which enables us to create a formal taxonomy of all existing dense DNN accelerators. Surprisingly, the loop transformations needed to create these hardware variants can be precisely and concisely represented by Halide's scheduling language. By modifying the Halide compiler to generate hardware, we create a system that can fairly compare these prior accelerators. As long as proper loop blocking schemes are used, and the hardware can support mapping replicated loops, many different hardware dataflows yield similar energy efficiency with good performance. This is because the loop blocking can ensure that most data references stay on-chip with good locality and the processing units have high resource utilization. How resources are allocated, especially in the memory system, has a large impact on energy and performance. By optimizing hardware resource allocation while keeping throughput constant, we achieve up to 4.2X energy improvement for Convolutional Neural Networks (CNNs), 1.6X and 1.8X improvement for Long Short-Term Memories (LSTMs) and multi-layer perceptrons (MLPs), respectively.},
	urldate = {2021-11-15},
	journal = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
	author = {Yang, Xuan and Gao, Mingyu and Liu, Qiaoyi and Setter, Jeff Ou and Pu, Jing and Nayak, Ankita and Bell, Steven Emberton and Cao, Kaidi and Ha, Heonjae and Raina, Priyanka and Kozyrakis, Christos and Horowitz, Mark},
	month = mar,
	year = {2020},
	note = {arXiv: 1809.04070},
	keywords = {C.1.4, C.3, C.4, Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {369--383},
	file = {arXiv Fulltext PDF:C\:\\Users\\Armeria\\Zotero\\storage\\QIIR9UD7\\Yang 等。 - 2020 - Interstellar Using Halide's Scheduling Language t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Armeria\\Zotero\\storage\\2674TKQE\\1809.html:text/html},
}

@article{chen_diannao_2014,
	title = {{DianNao}: a small-footprint high-throughput accelerator for ubiquitous machine-learning},
	volume = {42},
	issn = {0163-5964},
	shorttitle = {{DianNao}},
	url = {https://dl.acm.org/doi/10.1145/2654822.2541967},
	doi = {10.1145/2654822.2541967},
	abstract = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.
            Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.
            We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.},
	language = {en},
	number = {1},
	urldate = {2021-11-16},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
	month = apr,
	year = {2014},
	pages = {269--284},
}

@article{sze_efficient_2017,
	title = {Efficient {Processing} of {Deep} {Neural} {Networks}: {A} {Tutorial} and {Survey}},
	volume = {105},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Efficient {Processing} of {Deep} {Neural} {Networks}},
	url = {http://ieeexplore.ieee.org/document/8114708/},
	doi = {10.1109/JPROC.2017.2761740},
	language = {en},
	number = {12},
	urldate = {2021-11-16},
	journal = {Proceedings of the IEEE},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
	month = dec,
	year = {2017},
	pages = {2295--2329},
	file = {Sze 等。 - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf:C\:\\Users\\Armeria\\Zotero\\storage\\NFFKM6HV\\Sze 等。 - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf:application/pdf},
}
